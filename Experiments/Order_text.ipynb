{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyphen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# from transformers import AutoTokenizer #4.25.1 torch 1.7.1\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyphen\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyphen'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import os\n",
    "# from transformers import AutoTokenizer #4.25.1 torch 1.7.1\n",
    "import pyphen\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.pre_tokenizers import ByteLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_process='strict_model_ordered_data_reverse'\n",
    "path='./SlovakBabyLM/Curricullum_learning/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "load_json_path=f'{path}{folder_to_process}'\n",
    "save_path=f'{path}tok_bpe/{folder_to_process}_BPE/text_tokenizer.txt'\n",
    "tokenizer_dir = f\"{path}tok_bpe/{folder_to_process}_BPE\"\n",
    "# if you work with whole_data you need to put it into path\n",
    "\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def define_txt_file(load_path,save_path):\n",
    "    text=''\n",
    "    for subdir, dirs, files in os.walk(load_path):\n",
    "        for file in files: \n",
    "            file_path = os.path.join(subdir, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                if not \"_processed\" in file_path and not \"_reordered\" in file_path:\n",
    "                    # print(f'\\n Start Process: {file_path}')\n",
    "                    data = json.load(f)\n",
    "                    for i in data:\n",
    "                        if isinstance(i['page'],list):\n",
    "                            i['page']=' '.join(i['page'])\n",
    "                        text+=i['page']\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "\n",
    "text= define_txt_file(load_json_path,save_path)\n",
    "\n",
    "tokenizer.train(\n",
    "    [save_path],\n",
    "    vocab_size=60000, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "tokenizer_json_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "tokenizer.save(tokenizer_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(f\"{path}{folder_to_process}_BPE/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute word frequency and bi-gram frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(words):\n",
    "    return [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "\n",
    "\n",
    "def compute_frequency_words(file_path,full_word_freq,full_bi_freq,full_tokens_freq):\n",
    "    try:\n",
    "        if not \"_processed\" in file_path and not \"_reordered\" in file_path:\n",
    "            with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                print(f'Processed freq:{file_path}')\n",
    "                data = json.load(f)\n",
    "                for source in data:\n",
    "                    tokens_source=tokenizer.encode(source['page']).tokens\n",
    "                    #tokens freq\n",
    "                    tokens_freq = Counter(tokens_source)\n",
    "                    full_tokens_freq.update(tokens_freq)                          \n",
    "                    #source to words\n",
    "                    words = source['page'].split()\n",
    "                    words=[re.sub(r'[^\\p{L}\\p{N} ]+', '', item) for item in words]\n",
    "                    bi_gram=generate_bigrams(words)\n",
    "                    #bi_gram\n",
    "                    bi_freq = Counter(bi_gram)\n",
    "                    full_bi_freq.update(bi_freq)\n",
    "                    #individual_words\n",
    "                    word_freq = Counter(words)\n",
    "                    full_word_freq.update(word_freq)\n",
    "            # freq_dict[file_path]={'token_f':full_tokens_freq,'bi_f':full_bi_freq,'word_f':full_word_freq}\n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return full_word_freq,full_bi_freq,full_tokens_freq\n",
    "\n",
    "\n",
    "\n",
    "def bi_gram_freq(source,bi_freq):\n",
    "    source_all_bi_gram=[]\n",
    "    bi_gram=generate_bigrams(source)\n",
    "    for bi in bi_gram:\n",
    "        source_all_bi_gram.append(bi_freq[bi])\n",
    "    return source_all_bi_gram\n",
    "\n",
    "def tok_freq(s,tokens_freq):\n",
    "    source_all_tok=[]\n",
    "    tokens_source=tokenizer.encode(s['page']).tokens\n",
    "    for token in tokens_source:\n",
    "        source_all_tok.append(tokens_freq[token])\n",
    "    return source_all_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_word_freq = Counter()\n",
    "full_bi_freq = Counter()\n",
    "full_tokens_freq = Counter()\n",
    "for subdir, dirs, files in os.walk(f'{path}{folder_to_process}'):\n",
    "    for dir in dirs: \n",
    "        folder_path = os.path.join(subdir, dir)\n",
    "        if not '_reordered' in folder_path and not '_processed' in folder_path:\n",
    "            for files in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, files)\n",
    "                full_word_freq,full_bi_freq,full_tokens_freq=compute_frequency_words(file_path,full_word_freq,full_bi_freq,full_tokens_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed freq:/data/lubosk/diploma_thesis/data/wiki_full/wiki_full_data.json\n"
     ]
    }
   ],
   "source": [
    "full_word_freq = Counter()\n",
    "full_bi_freq = Counter()\n",
    "full_tokens_freq = Counter()\n",
    "for subdir, dirs, files in os.walk(f'{path}{folder_to_process}'):\n",
    "    for file in files: \n",
    "        file_path = os.path.join(subdir, file)\n",
    "        if not '_reordered' in file_path and not '_processed' in file_path:\n",
    "            full_word_freq,full_bi_freq,full_tokens_freq=compute_frequency_words(file_path,full_word_freq,full_bi_freq,full_tokens_freq )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed freq:/data/lubosk/diploma_thesis/strict_model_ordered_data_full_data/strict_model_ordered_data_full_data.json\n"
     ]
    }
   ],
   "source": [
    "full_word_freq = Counter()\n",
    "full_bi_freq = Counter()\n",
    "full_tokens_freq = Counter()\n",
    "\n",
    "full_word_freq,full_bi_freq,full_tokens_freq=compute_frequency_words(f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data.json',full_word_freq,full_bi_freq,full_tokens_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_counter(text):\n",
    "    pattern = r\"(?<!\\d)([;.?!])(?!\\d)(?![;.?!])\"\n",
    "    punk_sum=sum(1 for _ in re.finditer(pattern,text))\n",
    "    return punk_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spojky = [\n",
    "    \"a\",\"že\", \"i\",\"keby\", \"aby\",\"aj\", \"ak\",\"keď\", \"keďže\",\"ako\",\"akoby\",\"hoci\",\"ale\", \"alebo\",\"lebo\", \"ani\", \"iba\",\"tak\", \"takže\", \"teda\", \"totižto\", \"veď\", \"však\", \"žeby\"\n",
    "    \"avšak\", \"až\", \"ba\", \"bár\", \"beztak\", \"buď\",\"by\", \"či\", \"čím\",\"čoby\",'pričom'\n",
    "    \"čiže\", \"čo\", \"kým\", \"leda\", \"ledva\", \"len\", \"len čo\", \"totiž\",\n",
    "    \"lenže\",\"najprv\", \"nech\", \"než\",\"nielen\", \"no\", \"nuž\",\"pokiaľ\", \"pokým\", \"predsa\", \"preto\", \"pretože\", \"síce\",\"sotva\", \"sťa\", 'prípadne', 'poprípade', 'eventuálne',\n",
    "]\n",
    "len(spojky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len slová ktoré môžu byť predložky\n",
    "predlozky = [\n",
    "    \"bez\", \"cez\", \"do\", \"k\", \"medzi\", \"na\", \"nad\", \"o\", \"od\", \"okrem\", \"po\", \"pod\", \"pre\", \"pred\",\n",
    "    \"pri\", \"proti\", \"s\", \"skrz\", \"u\", \"v\", \"z\", \"za\",\n",
    "    \"ponad\", \"popod\", \"popred\", \"poza\", \"popri\", \"pomedzi\", \"znad\", \"spred\", \"zmedzi\",\n",
    "    \"spod\", \"spopred\", \"sponad\", \"spopod\", \"spoza\", \"spopri\", \"spomedzi\",\n",
    "    'zo','ku','voči','skrze','vo','so']\n",
    "\n",
    "len(predlozky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of dataset- create '_processed' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processed(file_path,word_f_counter,bi_f_counter,token_f_counter):\n",
    "    dic=pyphen.Pyphen(lang='sk_SK')\n",
    "    if not \"_processed\" in file_path and not \"_reordered\" in file_path:\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "            print(f'\\n Start Process: {file_path}')\n",
    "            data = json.load(f)\n",
    "        for i,s in enumerate(data):\n",
    "            if not s['page']:\n",
    "                continue\n",
    "            punct_count=punctuation_counter(s['page'])\n",
    "            source= s['page'].split()\n",
    "            syl_list=[]\n",
    "            w_len=[]\n",
    "            freq_list=[]\n",
    "            spojky_number=0\n",
    "            predlozky_number=0\n",
    "            for w in source: \n",
    "                w_without_dia=re.sub(r'[^\\p{L}\\p{N} ]+', '', w) \n",
    "                if w_without_dia in spojky:\n",
    "                    spojky_number+=1\n",
    "                if w_without_dia in predlozky:\n",
    "                    predlozky_number+=1\n",
    "                w_len.append(len(w_without_dia))\n",
    "                syl_list.append(len(dic.inserted(w).split(\"-\")))\n",
    "                if w_without_dia:\n",
    "                    freq_list.append(word_f_counter[w_without_dia])  \n",
    "\n",
    "            len_functional_words=len([s for s in source if re.search(r\"[A-Za-z0-9]\", s)])\n",
    "            try:\n",
    "                source_bi_freq=bi_gram_freq(source,bi_f_counter)\n",
    "                source_all_tok=tok_freq(s,token_f_counter)\n",
    "                s['index']=i\n",
    "                ## Frequency\n",
    "                s['average_source_rarity']=-sum(freq_list)/len(freq_list)\n",
    "                s['average_tok']=-sum(source_all_tok)/len(source_all_tok)\n",
    "                s['average_bi_gram']=-sum(source_bi_freq)/len(source_bi_freq)  \n",
    "                ## Grammar  \n",
    "                s['average_word_length']=sum(w_len)/len(w_len)\n",
    "                s['sylabelle/word']=sum(syl_list)/len_functional_words\n",
    "                s['spojky']=spojky_number/len_functional_words\n",
    "                s['predlozky']=predlozky_number/len_functional_words\n",
    "                s['punctuation_density']=-punct_count/len(source)\n",
    "            except Exception as e :\n",
    "                print(e) \n",
    "        new_path=file_path.split(\".\") \n",
    "        new_path=new_path[0]+'_processed.'+new_path[1]\n",
    "        with open(new_path, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/fairytales/fairytales.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/edu/edu.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/childes/childes.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/wiki/wiki.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/subs/subs.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model/lit/lit.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}'\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "            for dir in dirs: \n",
    "                folder_path = os.path.join(subdir, dir)\n",
    "                for files in os.listdir(folder_path):\n",
    "                    file_path = os.path.join(folder_path, files)\n",
    "                    create_processed(file_path,full_word_freq,full_bi_freq,full_tokens_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_full_data/strict_model_ordered_data_full_data.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data.json'\n",
    "create_processed(path,full_word_freq,full_bi_freq,full_tokens_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate _process: Create order of data in index form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text(ordering_index,ordering_folders,file_path,order_by):\n",
    "    # if \"_processed\" in file_path:\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        print(f'\\n Start Process: {file_path}')\n",
    "        data = json.load(f)\n",
    "        for s in data:\n",
    "            s.pop('page', None)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('index', inplace=True)\n",
    "    numeric_columns = df.select_dtypes(include=[\"number\"]).columns\n",
    "    #final order of files\n",
    "    ordering_folders[file_path]=df[numeric_columns]\n",
    "    #normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_columns]=scaler.fit_transform(df[numeric_columns])\n",
    "    # Combination\n",
    "    freq_group = numeric_columns[:3]   \n",
    "    gramma_group = numeric_columns[-5:]  \n",
    "    df[\"freq_group\"] = df[freq_group].sum(axis=1)\n",
    "    df[\"gramma_group\"] = df[gramma_group].sum(axis=1)\n",
    "    df['sum_freq_gramma']=df[\"freq_group\"]+df[\"gramma_group\"]\n",
    "    #sorting\n",
    "    sorted_df = df.sort_values(by=order_by)\n",
    "    index_order = sorted_df.index.tolist()\n",
    "    #final list\n",
    "    ordering_index[file_path]=index_order\n",
    "    return ordering_index,ordering_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_full_data/strict_model_ordered_data_full_data_processed.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json'\n",
    "order_by=\"sum_freq_gramma\"\n",
    "ordering_index={}\n",
    "ordering_folders={}\n",
    "\n",
    "ordering_index,ordering_folders=evaluate_text(ordering_index,ordering_folders,path,order_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/fairytales_full/fairytales_full.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/edu/edu.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/childes/childes.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/wiki_full/wiki_full.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/subs/subs.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/lit/lit.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}'\n",
    "order_by=\"sum_freq_gramma\"\n",
    "ordering_index={}\n",
    "ordering_folders={}\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for dir in dirs: \n",
    "        folder_path = os.path.join(subdir, dir)\n",
    "        for files in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, files)\n",
    "            ordering_index,ordering_folders=evaluate_text(ordering_index,ordering_folders,file_path,order_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,df in ordering_folders.items():\n",
    "    ordering_folders[k]=df.mean()\n",
    "df = pd.DataFrame(ordering_folders).T\n",
    "df_ranked=df.apply(lambda x: x.rank(method='first', ascending=True).astype(int))\n",
    "# df_ranked.index = df_ranked.index.map(lambda x: x.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked['Sum'] = df_ranked.sum(axis=1)\n",
    "df_ranked=df_ranked.sort_values(by='Sum', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/lubosk/diploma_thesis/strict_model_ordered_data/subs/subs.json',\n",
       " '/data/lubosk/diploma_thesis/strict_model_ordered_data/childes/childes.json',\n",
       " '/data/lubosk/diploma_thesis/strict_model_ordered_data/wiki_full/wiki_full.json',\n",
       " '/data/lubosk/diploma_thesis/strict_model_ordered_data/lit/lit.json',\n",
       " '/data/lubosk/diploma_thesis/strict_model_ordered_data/fairytales_full/fairytales_full.json',\n",
       " '/data/lubosk/diploma_thesis/strict_model_ordered_data/edu/edu.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ranked['Sum'] = df_ranked.sum(axis=1)\n",
    "df_ranked=df_ranked.sort_values(by='Sum', ascending=True)\n",
    "updated_paths = [path.replace('_processed', f'_reordered_{order_by}') for path in df_ranked.index.tolist()]\n",
    "updated_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_source_rarity</th>\n",
       "      <th>average_tok</th>\n",
       "      <th>average_bi_gram</th>\n",
       "      <th>average_word_length</th>\n",
       "      <th>sylabelle/word</th>\n",
       "      <th>spojky</th>\n",
       "      <th>predlozky</th>\n",
       "      <th>punctuation_density</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/subs/subs.json</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/childes/childes.json</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/wiki_full/wiki_full.json</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/fairytales_full/fairytales_full.json</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/edu/edu.json</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/lit/lit.json</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    average_source_rarity  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      6   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      4   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      3   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                      5   \n",
       "\n",
       "                                                    average_tok  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            5   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            6   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            4   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...            3   \n",
       "\n",
       "                                                    average_bi_gram  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                3   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                4   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                5   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                6   \n",
       "\n",
       "                                                    average_word_length  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    6   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    3   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    5   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    4   \n",
       "\n",
       "                                                    sylabelle/word  spojky  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               3       5   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               1       3   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               6       1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               2       6   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               5       2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...               4       4   \n",
       "\n",
       "                                                    predlozky  \\\n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          2   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          1   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          4   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          3   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          6   \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...          5   \n",
       "\n",
       "                                                    punctuation_density  Sum  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    2   17  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    1   21  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    6   29  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    3   31  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    5   35  \n",
       "/data/lubosk/diploma_thesis/strict_model_ordere...                    4   35  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering dictionaries Create _reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def order_dict_in_folders(file_path,ordering_index,order_by):    \n",
    "    # if \"_processed\" in file_path:\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        print(f'\\n Start Process: {file_path}')\n",
    "        data = json.load(f)\n",
    "    try:\n",
    "        ordering_index[file_path]=[x for x in ordering_index[file_path] if not math.isnan(x)]\n",
    "        if isinstance(ordering_index[file_path][0],float):\n",
    "            ordering_index[file_path]=[int(x) for x in ordering_index[file_path]]\n",
    "        reordered_data=[data[i] for i in ordering_index[file_path]]\n",
    "        # if \"_processed\" in file_path:\n",
    "        #     new_reorded_path=file_path.replace('_processed',f'_reordered_{order_by}')\n",
    "        # else:\n",
    "        file_path_split= file_path.split('.')\n",
    "        new_reorded_path=file_path_split[0] + f'_reordered_{order_by}'+file_path_split[1]\n",
    "        with open(new_reorded_path, \"w\", encoding=\"utf8\") as f_reorded:\n",
    "            json.dump(reordered_data, f_reorded, ensure_ascii=False, indent=4)\n",
    "    except:\n",
    "         print('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data/fairytales_full/fairytales_full.json\n",
      "e\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'new_reorded_path' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[1;32m      7\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, files)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43morder_dict_in_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mordering_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43morder_by\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m, in \u001b[0;36morder_dict_in_folders\u001b[0;34m(file_path, ordering_index, order_by)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mnew_reorded_path\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_reorded:\n\u001b[1;32m     21\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(reordered_data, f_reorded, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'new_reorded_path' referenced before assignment"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}'\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "            for dir in dirs: \n",
    "                folder_path = os.path.join(subdir, dir)\n",
    "                for files in os.listdir(folder_path):\n",
    "                    file_path = os.path.join(folder_path, files)\n",
    "                    order_dict_in_folders(file_path,ordering_index,order_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_full_data/strict_model_ordered_data_full_data_processed.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json'\n",
    "\n",
    "order_dict_in_folders(path,ordering_index,order_by)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
