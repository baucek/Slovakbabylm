{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a478ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import os\n",
    "# from transformers import AutoTokenizer #4.25.1 torch 1.7.1\n",
    "import pyphen\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.pre_tokenizers import ByteLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_process='strict_model_ordered_data_reverse'\n",
    "path='./SlovakBabyLM/Curricullum_learning/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753ed59",
   "metadata": {},
   "source": [
    "### CREATE WHOLE DATA JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041fc1b",
   "metadata": {},
   "source": [
    "Create whole json file from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73805843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/fairytales_full/fairytales_full.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/edu/edu.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/childes/childes.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/wiki_full/wiki_full.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/subs/subs.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/lit/lit.json\n"
     ]
    }
   ],
   "source": [
    "full_json_text=[]\n",
    "for subdir, dirs, files in os.walk(f'{path}{folder_to_process}'):\n",
    "    for dir in dirs: \n",
    "        folder_path = os.path.join(subdir, dir)\n",
    "        source=0\n",
    "        for files in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, files)\n",
    "            if '_processed' not in file_path and '_reordered' not in file_path :\n",
    "                print(file_path)\n",
    "                with open(file_path,encoding='utf-8') as data_file: \n",
    "                    data = json.load(data_file)\n",
    "                    full_json_text.extend(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e995ac",
   "metadata": {},
   "source": [
    "Create whole json file from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_json_text=[]\n",
    "for subdir, dirs, files in os.walk(f'{data}/wiki'):\n",
    "    for file in files: \n",
    "        file_path = os.path.join(subdir, file)\n",
    "        source=0\n",
    "        print(file_path)\n",
    "        with open(file_path,encoding='utf-8') as data_file: \n",
    "            data = json.load(data_file)\n",
    "            full_json_text.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json','a',encoding=\"utf8\") as f_reorded:\n",
    "    json.dump(full_json_text, f_reorded, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18099c4",
   "metadata": {},
   "source": [
    "Reshuffle of JSon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text=''\n",
    "full_json_text=random.sample(full_json_text,len(full_json_text))\n",
    "for i in full_json_text:\n",
    "    full_text += f\"{i['page'] }\"\n",
    "with open(f'{path}{folder_to_process}_results/{folder_to_process}_randomize.txt','a',encoding=\"utf8\") as f:\n",
    "    f.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ff33b",
   "metadata": {},
   "source": [
    "Delete evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ccf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json','r',encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_data = [{\"url\": item[\"url\"], \"page\": item[\"page\"]} for item in data]\n",
    "\n",
    "with open(f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7b5fb",
   "metadata": {},
   "source": [
    "Delete index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/fairytales_full/fairytales_full.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/edu/edu.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/childes/childes.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/wiki_full/wiki_full.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/subs/subs.json\n",
      "/data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/lit/lit.json\n"
     ]
    }
   ],
   "source": [
    "path=f'{path}{folder_to_process}'\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for dir in dirs: \n",
    "        folder_path = os.path.join(subdir, dir)\n",
    "        for files in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, files)\n",
    "            print(file_path)\n",
    "            with open(file_path,'r',encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                for index, dictionaries in enumerate(data):\n",
    "                    dictionaries['index']= index\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json','r',encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for index, dictionaries in enumerate(data):\n",
    "    dictionaries['index']= index\n",
    "\n",
    "with open(f'{path}{folder_to_process}_full_data/{folder_to_process}_full_data_processed.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56ad42",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853662ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/fairytales_full/fairytales_full.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/edu/edu.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/childes/childes.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/wiki_full/wiki_full.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/subs/subs.json\n",
      "\n",
      " Start Process: /data/lubosk/diploma_thesis/strict_model_ordered_data_reverse/lit/lit.json\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "load_json_path=f'{path}{folder_to_process}'\n",
    "save_path=f'/data/lubosk/diploma_thesis/tok_bpe/{folder_to_process}_BPE/text_tokenizer.txt'\n",
    "tokenizer_dir = f\"/data/lubosk/diploma_thesis/tok_bpe/{folder_to_process}_BPE\"\n",
    "\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def define_txt_file(load_path,save_path):\n",
    "    text=''\n",
    "    for subdir, dirs, files in os.walk(load_path):\n",
    "        for dir in dirs: \n",
    "            folder_path = os.path.join(subdir, dir)\n",
    "            for files in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, files)\n",
    "                with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                    if not \"_processed\" in file_path and not \"_reordered\" in file_path:\n",
    "                        print(f'\\n Start Process: {file_path}')\n",
    "                        data = json.load(f)\n",
    "                        for i in data:\n",
    "                            if isinstance(i['page'],list):\n",
    "                                i['page']=' '.join(i['page'])\n",
    "                            text+=i['page']\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "\n",
    "define_txt_file(load_json_path,save_path)\n",
    "\n",
    "tokenizer.train(\n",
    "    [save_path],\n",
    "    vocab_size=60000, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "tokenizer_json_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "tokenizer.save(tokenizer_json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576f264",
   "metadata": {},
   "source": [
    "# Choose randomly from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f2cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import random\n",
    "import regex as re\n",
    "folder_to_process='evaluation_data'\n",
    "condition_files='_reordered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51694f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_f(text):\n",
    "    text=re.sub(r'([.,!?;:\\'\\\"])\\n+\\1', r'\\1',text)\n",
    "    text=re.sub(r'([.,!?;:\\'\\\"])\\s+\\1', r'\\1', text)\n",
    "    text = re.sub(r'([!?;])\\1+', r'\\1', text)  \n",
    "    text=re.sub(r' +',r' ',text)\n",
    "    text=re.sub(r'(?<=\\n)\\s*\\d+\\s*(?=\\n)',r'\\n',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26504f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/data/subs/json_subtitles.json\n",
      "additional: 0\n"
     ]
    }
   ],
   "source": [
    "# where is data taken and how many \n",
    "paths=[\n",
    "        {'size_words':470_000,'path':f'{path}childes'},\n",
    "        {'size_words':2_300_000,'path':f'{path}wiki_full'},\n",
    "        {'size_words':910_000,'path':f'{path}fairytales_full'},\n",
    "        {'size_words':4_000_000,'path':f'{path}subs'},\n",
    "        {'size_words':1_304_000,'path':f'{path}edu'},\n",
    "        {'size_words':990_000,'path':f'{path}lit'},\n",
    "       ]\n",
    "for path in paths:\n",
    "    for subdir, dirs, files in os.walk(path['path']):\n",
    "        files=[item for item in files if '_processed' not in item and '_reordered' not in item]\n",
    "        counter= path['size_words']/len(files)\n",
    "        json_list=[]\n",
    "        additional=0\n",
    "        for file in files: \n",
    "            file_path = os.path.join(subdir, file)\n",
    "            print(file_path)\n",
    "            with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "            length_data=0\n",
    "            selected_indices = set(range(0, len(data)))\n",
    "            try: \n",
    "                while selected_indices and length_data <= counter + additional:\n",
    "                    random_number = random.choice(tuple(selected_indices))\n",
    "                    selected_indices.remove(random_number)\n",
    "                    data[random_number]['page']=preprocess_text_f(data[random_number]['page'])\n",
    "                    json_list.append(data[random_number])\n",
    "                    length_data=length_data+len(data[random_number]['page'].split(' '))\n",
    "                if len(selected_indices) ==0:\n",
    "                    parts = subdir.split('/')\n",
    "                    print(f'not enough text for size_words only {length_data} for {parts[-1]}:{file}')\n",
    "                    additional= counter-length_data +additional\n",
    "                else: \n",
    "                    additional=0\n",
    "                    print(f'additional: {additional}')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        parts = subdir.split('/')\n",
    "        parts[-2] =  folder_to_process\n",
    "        new_path = '/'.join(parts)\n",
    "        with open(f'{new_path}/{parts[-1]}.json', \"a\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758edba0",
   "metadata": {},
   "source": [
    "# Choose by order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26a9a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import random\n",
    "folder_to_process='strict_model_ordered_data_reverse'\n",
    "condition_files='_reordered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/data/childes/final_conv_reordered_sum_freq_gramma.json\n",
      "/data/lubosk/diploma_thesis/data/wiki_full/wiki_full_data_reordered_sum_freq_gramma.json\n",
      "/data/lubosk/diploma_thesis/data/fairytales_full/fairytales_full_data_reordered_sum_freq_gramma.json\n",
      "/data/lubosk/diploma_thesis/data/subs/json_subtitles_reordered_sum_freq_gramma.json\n",
      "/data/lubosk/diploma_thesis/data/edu/tahaky_referaty-prep_reordered_sum_freq_gramma.json\n",
      "/data/lubosk/diploma_thesis/data/lit/test_json_files_new_proc_reordered_sum_freq_gramma.json\n"
     ]
    }
   ],
   "source": [
    "paths=[\n",
    "        {'size_words':470_000,'path':f'{path}childes'},\n",
    "        {'size_words':2_300_000,'path':f'{path}wiki_full'},\n",
    "        {'size_words':910_000,'path':f'{path}fairytales_full'},\n",
    "        {'size_words':4_000_000,'path':f'{path}subs'},\n",
    "        {'size_words':1_304_000,'path':f'{path}edu'},\n",
    "        {'size_words':990_000,'path':f'{path}lit'},\n",
    "       ]\n",
    "for path in paths:\n",
    "    for subdir, dirs, files in os.walk(path['path']):\n",
    "        for file in files:\n",
    "            if condition_files in file:  \n",
    "                file_path = os.path.join(subdir, file)\n",
    "                json_list=[]\n",
    "                length_data=0\n",
    "                print(file_path)\n",
    "                with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                    data = json.load(f)\n",
    "                data.reverse()\n",
    "                for source in data:\n",
    "                    if length_data <= path['size_words']:\n",
    "                        length_data+= len(source['page'].split())\n",
    "                        json_list.append(source)\n",
    "                parts = subdir.split('/')\n",
    "                parts[-2] =  folder_to_process\n",
    "                new_path = '/'.join(parts)\n",
    "                with open(f'{new_path}/{parts[-1]}.json', \"a\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(json_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c653d1",
   "metadata": {},
   "source": [
    "Counter of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strict_model_ordered_data'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=0\n",
    "for source in data:\n",
    "    a+=len(source['page'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e470083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8acb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/data/fairytales_full/fairytales_full_data.json\n",
      "5,112,961\n",
      "6895\n",
      "/data/lubosk/diploma_thesis/data/fairytales_full/fairytales_full_data_processed.json\n",
      "5,112,961\n",
      "6895\n",
      "/data/lubosk/diploma_thesis/data/fairytales_full/fairytales_full_data_reordered_sum_freq_gramma.json\n",
      "5,112,961\n",
      "6895\n",
      "/data/lubosk/diploma_thesis/data/fairytales/zlatyfond_preprocessed.json\n",
      "671,388\n",
      "293\n",
      "/data/lubosk/diploma_thesis/data/fairytales/PDF_fairytales.json\n",
      "509,365\n",
      "30\n",
      "/data/lubosk/diploma_thesis/data/fairytales/created_fairytales.json\n",
      "1,786,974\n",
      "3094\n",
      "/data/lubosk/diploma_thesis/data/fairytales/rozpravky_online_preprocessed.json\n",
      "43,636\n",
      "87\n",
      "/data/lubosk/diploma_thesis/data/fairytales/svetrozpravok_preprocessed.json\n",
      "38,773\n",
      "70\n",
      "/data/lubosk/diploma_thesis/data/fairytales/readmio.json\n",
      "358,381\n",
      "1537\n",
      "/data/lubosk/diploma_thesis/data/fairytales/sikovnamamina_preprocessed.json\n",
      "41,392\n",
      "36\n",
      "/data/lubosk/diploma_thesis/data/fairytales/zones_preprocessed.json\n",
      "1,359,908\n",
      "1058\n",
      "/data/lubosk/diploma_thesis/data/fairytales/rozpravkozem_2.json\n",
      "303,144\n",
      "690\n",
      "/data/lubosk/diploma_thesis/data/edu/tahaky_referaty-prep_processed.json\n",
      "14,954,348\n",
      "17214\n",
      "/data/lubosk/diploma_thesis/data/edu/tahaky_referaty-prep_reordered_sum_freq_gramma.json\n",
      "14,954,348\n",
      "17214\n",
      "/data/lubosk/diploma_thesis/data/edu/tahaky_referaty-prep.json\n",
      "14,954,348\n",
      "17214\n",
      "/data/lubosk/diploma_thesis/data/childes/final_conv_reordered_sum_freq_gramma.json\n",
      "1,732,601\n",
      "30066\n",
      "/data/lubosk/diploma_thesis/data/childes/final_conv_processed.json\n",
      "1,732,601\n",
      "30070\n",
      "/data/lubosk/diploma_thesis/data/childes/final_conv.json\n",
      "1,732,601\n",
      "30070\n",
      "/data/lubosk/diploma_thesis/data/wiki_full/wiki_full_data_processed.json\n",
      "58,008,943\n",
      "150463\n",
      "/data/lubosk/diploma_thesis/data/wiki_full/wiki_full_data_reordered_sum_freq_gramma.json\n",
      "58,008,943\n",
      "150463\n",
      "/data/lubosk/diploma_thesis/data/wiki_full/wiki_full_data.json\n",
      "58,008,943\n",
      "150463\n",
      "/data/lubosk/diploma_thesis/data/wiki/slovensky_jaz_processed.json\n",
      "2,170,375\n",
      "5281\n",
      "/data/lubosk/diploma_thesis/data/wiki/wiki-chemia_processed.json\n",
      "3,219,831\n",
      "8552\n",
      "/data/lubosk/diploma_thesis/data/wiki/hudba_processed.json\n",
      "4,835,216\n",
      "12570\n",
      "/data/lubosk/diploma_thesis/data/wiki/sport_processed.json\n",
      "2,763,223\n",
      "6588\n",
      "/data/lubosk/diploma_thesis/data/wiki/wiki-fyzika_processed.json\n",
      "723,209\n",
      "1741\n",
      "/data/lubosk/diploma_thesis/data/wiki/dejepis_processed.json\n",
      "6,483,417\n",
      "16616\n",
      "/data/lubosk/diploma_thesis/data/wiki/wiki-informatika_processed.json\n",
      "553,537\n",
      "1186\n",
      "/data/lubosk/diploma_thesis/data/wiki/final_adjust_processed.json\n",
      "31,816,556\n",
      "86458\n",
      "/data/lubosk/diploma_thesis/data/wiki/wiki-biologia_processed.json\n",
      "1,396,678\n",
      "3414\n",
      "/data/lubosk/diploma_thesis/data/wiki/output_processed.json\n",
      "688,245\n",
      "866\n",
      "/data/lubosk/diploma_thesis/data/wiki/output_wiki_predmety_processed.json\n",
      "3,358,656\n",
      "7191\n",
      "/data/lubosk/diploma_thesis/data/subs/json_subtitles_processed.json\n",
      "53,611,749\n",
      "8962\n",
      "/data/lubosk/diploma_thesis/data/subs/json_subtitles.json\n",
      "53,611,749\n",
      "8962\n",
      "/data/lubosk/diploma_thesis/data/subs/json_subtitles_reordered_sum_freq_gramma.json\n",
      "53,611,749\n",
      "8962\n",
      "/data/lubosk/diploma_thesis/data/lit/test_json_files_new_proc_reordered_sum_freq_gramma.json\n",
      "7,673,184\n",
      "271\n",
      "/data/lubosk/diploma_thesis/data/lit/test_json_files_new_proc_processed.json\n",
      "7,673,184\n",
      "271\n",
      "/data/lubosk/diploma_thesis/data/lit/test_json_files_new_proc.json\n",
      "7,673,184\n",
      "271\n",
      "Total: 486,403,262\n"
     ]
    }
   ],
   "source": [
    "total_word_count=0\n",
    "for subdir, dirs, files in os.walk(f'{path}'):\n",
    "    for dir in dirs:\n",
    "        folder_path = os.path.join(subdir, dir)\n",
    "        source=0\n",
    "        for files in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, files)\n",
    "            with open(file_path,encoding='utf-8') as data_file:    \n",
    "                data = json.load(data_file)\n",
    "                for i in data:\n",
    "                    source= source+len(i['page'].split())\n",
    "                print(file_path)\n",
    "                print(\"{:,}\".format(source))\n",
    "                print(len(data))\n",
    "                total_word_count=total_word_count+source\n",
    "                source=0\n",
    "print(f'Total: {\"{:,}\".format(total_word_count)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
