{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertConfig, BertForMaskedLM, Trainer, TrainingArguments,PreTrainedTokenizerFast\n",
    "from datasets import Dataset,DatasetDict\n",
    "import json\n",
    "import torch \n",
    "import wandb\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/bmxia6es?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0bf89b4d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(mode=\"disabled\")\n",
    "# !PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_train='strict_model'\n",
    "file_to_load='3_full_data__sum_freq_gramma__without_mask'\n",
    "path='./SlovakBabyLM/Curricullum_learning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer.from_file(f\"{path}tok_bpe/{file_to_train}_BPE/tokenizer.json\")\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=test_tokenizer,\n",
    "    return_token_type_ids=False,\n",
    "    truncation=True,\n",
    "    return_special_tokens_mask=True,\n",
    "    mask_token='[MASK]',\n",
    "    pad_token='[PAD]'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=60000,\n",
    "    hidden_size=84,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=1446,\n",
    "    hidden_dropout_prob=0.15,\n",
    "    attention_probs_dropout_prob=0.3,  \n",
    "    hidden_act=\"gelu_new\",\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model.tokenizer = hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lubosk/.local/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{path}saved_model/{file_to_train}/{file_to_load}/checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs= 7,\n",
    "    per_device_train_batch_size =32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy= \"steps\",       \n",
    "    eval_steps= 1000,                   \n",
    "    save_steps= 1000,                   \n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,      \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    bf16=True,                        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load masked and unmasked text\n",
    "with open(f\"{path}{file_to_train}_results/{file_to_load}/final_masked_text.txt\", \"r\", encoding=\"utf-8\") as f_masked:\n",
    "    masked_text = f_masked.read()\n",
    "\n",
    "with open(f\"{path}{file_to_train}_results/{file_to_load}/final_not_masked_text.txt\", \"r\", encoding=\"utf-8\") as f_unmasked:\n",
    "    unmasked_text = f_unmasked.read()\n",
    "\n",
    "\n",
    "batch_masked_text_list=masked_text.split('ð¡¨¸')\n",
    "batch_unmasked_text_list=unmasked_text.split('ð¡¨¸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133064"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_masked_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_unmasked_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(text_masked,text_unmasked):\n",
    "    part_size = len(text_unmasked) // 10\n",
    "\n",
    "    parts = [text_unmasked[i*part_size:(i+1)*part_size] for i in range(10)]\n",
    "    parts_m = [text_masked[i*part_size:(i+1)*part_size] for i in range(10)]\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    evaluate_data= []\n",
    "    \n",
    "    for unmasked_part, masked_part in zip(parts,parts_m):\n",
    "        split_point = int(len(unmasked_part) * 0.8)\n",
    "        \n",
    "        train_data.extend([{\"unmasked_text\": unmask,\"masked_text\": mask} for unmask,mask in zip(unmasked_part[:split_point],masked_part[:split_point])])\n",
    "\n",
    "        twenty_percent_un=unmasked_part[split_point:]\n",
    "        twenty_percent_ma=masked_part[split_point:]\n",
    "        split_point_t_e = int(len(twenty_percent_ma) * 0.5) \n",
    "\n",
    "        test_data.extend([{\"unmasked_text\": unmask,\"masked_text\": mask}for unmask,mask in zip(twenty_percent_un[:split_point_t_e],twenty_percent_ma[:split_point_t_e])])\n",
    "        evaluate_data.extend([{\"unmasked_text\": unmask,\"masked_text\": mask}for unmask,mask in zip(twenty_percent_un[split_point_t_e:],twenty_percent_ma[split_point_t_e:])])\n",
    "\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': Dataset.from_list(train_data),\n",
    "        'test': Dataset.from_list(test_data),\n",
    "        'evaluate': Dataset.from_list(evaluate_data)\n",
    "    })\n",
    "\n",
    "final_datasets = split_dataset(batch_masked_text_list,batch_unmasked_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106445/106445 [00:47<00:00, 2239.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13305/13305 [00:05<00:00, 2376.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13310/13310 [00:05<00:00, 2264.42 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    outputs = hf_tokenizer(\n",
    "        examples[\"masked_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False, \n",
    "    )\n",
    "    \n",
    "    labels = hf_tokenizer(\n",
    "        examples[\"unmasked_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False, \n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Convert to tensors\n",
    "    mask_token_id = hf_tokenizer.mask_token_id\n",
    "    for i, batch_token in enumerate(outputs[\"input_ids\"]):\n",
    "        for j, token_id in enumerate(batch_token):\n",
    "            if token_id != mask_token_id:\n",
    "                labels[i][j] = -100\n",
    "    outputs[\"labels\"] = labels\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=[\"unmasked_text\",\"masked_text\"])\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlubos-kris\u001b[0m (\u001b[33mlubos-kris-comenius-university-in-bratislava\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/lubosk/diploma_thesis/wandb/run-20250409_131027-i6htnz7r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks/runs/i6htnz7r' target=\"_blank\">7_full_data__sum_freq_gramma__max_freq</a></strong> to <a href='https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks' target=\"_blank\">https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks/runs/i6htnz7r' target=\"_blank\">https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks/runs/i6htnz7r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lubos-kris-comenius-university-in-bratislava/small_strict_model_masks/runs/i6htnz7r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2ec6c6f4c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.init(\n",
    "    project=f\"{file_to_train}\",\n",
    "    name=f'{file_to_load}',\n",
    "    config=training_args.to_dict()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11648' max='11648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11648/11648 1:53:16, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.315100</td>\n",
       "      <td>8.413993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.801300</td>\n",
       "      <td>8.047735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.727800</td>\n",
       "      <td>8.002670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.728700</td>\n",
       "      <td>7.972606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.685400</td>\n",
       "      <td>7.951394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>7.665300</td>\n",
       "      <td>7.941825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>7.662800</td>\n",
       "      <td>7.926215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>7.644900</td>\n",
       "      <td>7.919032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>7.628500</td>\n",
       "      <td>7.913572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>7.639100</td>\n",
       "      <td>7.908670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>7.633400</td>\n",
       "      <td>7.907313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "/data/lubosk/diploma_thesis/evaluation_python/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    }
   ],
   "source": [
    "save_path=f\"{path}saved_model/{file_to_train}/{file_to_load}\"\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"evaluate\"], \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "hf_tokenizer.save_pretrained(save_path)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}{file_to_train}_results/{file_to_load}/final_not_masked_text.txt\", \"r\", encoding=\"utf-8\") as text:\n",
    "    unmasked_text = text.read()\n",
    "\n",
    "batch_unmasked_text_list = unmasked_text.split('ð¡¨¸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m         evaluate_data\u001b[38;5;241m.\u001b[39mextend([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munmasked_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: text} \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m twenty_percent[split_point_t_e:]])\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(train_data),\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(test_data),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(evaluate_data)\n\u001b[1;32m     25\u001b[0m     })\n\u001b[0;32m---> 27\u001b[0m final_datasets \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_unmasked_text_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36msplit_dataset\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m     test_data\u001b[38;5;241m.\u001b[39mextend([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munmasked_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: text} \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m twenty_percent[:split_point_t_e]])\n\u001b[1;32m     19\u001b[0m     evaluate_data\u001b[38;5;241m.\u001b[39mextend([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munmasked_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: text} \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m twenty_percent[split_point_t_e:]])\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDatasetDict\u001b[49m({\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(train_data),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(test_data),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate\u001b[39m\u001b[38;5;124m'\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_list(evaluate_data)\n\u001b[1;32m     25\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetDict' is not defined"
     ]
    }
   ],
   "source": [
    "def split_dataset(text_list):\n",
    "    part_size = len(text_list) // 10\n",
    "    parts = [text_list[i*part_size:(i+1)*part_size] for i in range(5)]\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    evaluate_data= []\n",
    "    \n",
    "    for part in parts:\n",
    "        split_point = int(len(part) * 0.8)\n",
    "        \n",
    "        train_data.extend([{\"unmasked_text\": text} for text in part[:split_point]])\n",
    "\n",
    "        twenty_percent=part[split_point:]\n",
    "        split_point_t_e = int(len(twenty_percent) * 0.5)     \n",
    "\n",
    "        test_data.extend([{\"unmasked_text\": text} for text in twenty_percent[:split_point_t_e]])\n",
    "\n",
    "        evaluate_data.extend([{\"unmasked_text\": text} for text in twenty_percent[split_point_t_e:]])\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': Dataset.from_list(train_data),\n",
    "        'test': Dataset.from_list(test_data),\n",
    "        'evaluate': Dataset.from_list(evaluate_data)\n",
    "    })\n",
    "\n",
    "final_datasets = split_dataset(batch_unmasked_text_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22d572fde444eee823ed6db555dbd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757a50ce00c34a4593b19ef62009b573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea30840eb37b44ebb0c48f4d6eba86af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return hf_tokenizer(\n",
    "        examples[\"unmasked_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False, \n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=[\"unmasked_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = wandb.init(\n",
    "    project=f'{file_to_train}',\n",
    "    name=f'{file_to_load}',\n",
    "    config=training_args.to_dict()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10801' max='10801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10801/10801 1:26:42, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.634000</td>\n",
       "      <td>8.518370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.188000</td>\n",
       "      <td>8.112309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>8.114900</td>\n",
       "      <td>8.024786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>8.047400</td>\n",
       "      <td>7.961143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.973400</td>\n",
       "      <td>7.898160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>7.919700</td>\n",
       "      <td>7.845847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>7.896900</td>\n",
       "      <td>7.820701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>7.859100</td>\n",
       "      <td>7.785871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>7.869200</td>\n",
       "      <td>7.781829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>7.818700</td>\n",
       "      <td>7.763749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/data/lubosk/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='193' max='193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [193/193 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function wandb.sdk.wandb_init._WandbInit.make_disabled_run.<locals>.<lambda>(*_, **__)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path=f\"{path}saved_model/{file_to_train}/{file_to_load}\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"evaluate\"], \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()\n",
    "model.save_pretrained(save_path)\n",
    "hf_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "test_results =trainer.evaluate(tokenized_datasets[\"test\"]) \n",
    "# test_results = test_results.get(\"test_results\", None) \n",
    "# with open(f\"{loss_path}/test_results.json\", \"w\") as f:\n",
    "#     json.dump(test_results, f)\n",
    "run.finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many [MASK] are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}{file_to_train}_results/{file_to_load}/final_not_masked_text.txt\", \"r\", encoding=\"utf-8\") as text:\n",
    "    unmasked_text = text.read()\n",
    "\n",
    "batch_unmasked_text_list = unmasked_text.split('ð¡¨¸')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(text_list):\n",
    "    data = []\n",
    "    for text in text_list:\n",
    "        data.append({\"unmasked_text\": text})\n",
    "    return DatasetDict({\n",
    "        'train': Dataset.from_list(data)\n",
    "    })\n",
    "\n",
    "test_data = split_dataset(batch_unmasked_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer.from_file(f\"{path}tok_bpe/{file_to_train}_BPE/tokenizer.json\")\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=test_tokenizer,\n",
    "    return_token_type_ids=False,\n",
    "    truncation=True,\n",
    "    return_special_tokens_mask=True,\n",
    "    mask_token='[MASK]',\n",
    "    pad_token='[PAD]'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de45ae5fd16f4d63b50cf46350048dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return hf_tokenizer(\n",
    "        examples[\"unmasked_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False, \n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = test_data.map(tokenize_function, batched=True, remove_columns=[\"unmasked_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53b1c73228947219dfe6ba35ac608ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_tensors(example):\n",
    "    example['input_ids'] = torch.tensor(example['input_ids'])\n",
    "    example['attention_mask'] = torch.tensor(example['attention_mask'])\n",
    "    example['special_tokens_mask'] = torch.tensor(example['special_tokens_mask'])\n",
    "    return example\n",
    "tr=tokenized_datasets['train']\n",
    "train_data = tr.map(convert_to_tensors)\n",
    "\n",
    "a={'input_ids':train_data['input_ids'],'attention_mask':train_data['attention_mask'], 'special_tokens_mask':train_data['special_tokens_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_a= data_collator([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2657863"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(col_a['labels'] != -100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2656394"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(col_a['labels'] != -100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2658517"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(col_a['labels'] != -100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2659281"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(col_a['labels'] != -100).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
