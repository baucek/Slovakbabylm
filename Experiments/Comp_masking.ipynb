{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import os\n",
    "# from transformers import AutoTokenizer #4.25.1 torch 1.7.1\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "import pyphen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./SlovakBabyLM/Curricullum_learning/'\n",
    "file_to_train='strict_model_ordered_data_reverse'\n",
    "file_to_save='1_full_data__without_ord__without_mask'\n",
    "\n",
    "# paths=[f'{path}strict_model_individual_datasets/wiki_full/wiki_full.json']\n",
    "# txt_paths=[f'{path}strict_model_results/7_full_data__sum_freq_gramma__max_freq/final_not_masked_text.txt']\n",
    "# txt_paths=[f'{path}strict_model_ordered_data_results/1_full_data__without_ord__without_mask/randomize.txt']\n",
    "\n",
    "# strict_model\n",
    "# paths=[f'{path}strict_model/childes/childes_reordered_sum_freq_gramma.json',\n",
    "#  f'{path}strict_model/subs/subs_reordered_sum_freq_gramma.json',\n",
    "#  f'{path}strict_model/fairytales/fairytales_reordered_sum_freq_gramma.json',\n",
    "#  f'{path}strict_model/lit/lit_reordered_sum_freq_gramma.json',\n",
    "#  f'{path}strict_model/edu/edu_reordered_sum_freq_gramma.json',\n",
    "#  f'{path}strict_model/wiki/wiki_reordered_sum_freq_gramma.json']\n",
    "\n",
    "#strict_model_ordered\n",
    "# paths=[f'{path}strict_model_ordered_data/subs/subs.json',\n",
    "#  f'{path}strict_model_ordered_data/childes/childes.json',\n",
    "#  f'{path}strict_model_ordered_data/wiki_full/wiki_full.json',\n",
    "#  f'{path}strict_model_ordered_data/lit/lit.json',\n",
    "#  f'{path}strict_model_ordered_data/fairytales_full/fairytales_full.json',\n",
    "#  f'{path}strict_model_ordered_data/edu/edu.json']\n",
    "\n",
    "# paths=[f'{path}strict_model_ordered_data_full_data/strict_model_ordered_data_full_data_processed_reordered_sum_freq_gramma.json']\n",
    "\n",
    "# strict_model_ordered_reverse\n",
    "paths=[f'{path}strict_model_ordered_data_reverse_full_data/strict_model_ordered_data_reverse_full_data_processed.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer= Tokenizer.from_file(f\"{path}tok_bpe/{file_to_train}_BPE/tokenizer.json\")\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=test_tokenizer, return_special_tokens_mask=True, mask_token='[MASK]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unmasked text and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=128):\n",
    "    if text:\n",
    "        encoded = hf_tokenizer.encode_plus(text, add_special_tokens=False, return_offsets_mapping=True)\n",
    "        offsets = encoded[\"offset_mapping\"] \n",
    "        divisible_offsets = [offsets[i] for i in range(max_tokens, len(offsets), max_tokens)]\n",
    "        if divisible_offsets:\n",
    "            chunks = []\n",
    "            start_offset=0\n",
    "            for offset in divisible_offsets:\n",
    "                chunks.append(text[start_offset:offset[-1]])\n",
    "                start_offset=offset[-1]\n",
    "            chunks.append(text[offset[-1]:])  \n",
    "            return chunks\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_json(paths,shuffle):\n",
    "    text=''\n",
    "    for file_path in paths:\n",
    "        print(file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "        if shuffle:\n",
    "            data = random.sample(data, len(data))        \n",
    "        for i in data:\n",
    "            chunks=chunk_text(i['page'])\n",
    "            source=' 𡨸 '.join(chunks)\n",
    "            text=text+source \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_text(paths,shuffle):\n",
    "    for file_path in paths:\n",
    "        print(file_path)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        if shuffle:\n",
    "            content = random.sample(content, len(content))\n",
    "        chunks=chunk_text(content)\n",
    "        text=' 𡨸 '.join(chunks)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create unmasked text: for file and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=create_text_from_json(paths,shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_text_from_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_text_from_text\u001b[49m(paths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_text_from_text' is not defined"
     ]
    }
   ],
   "source": [
    "text=create_text_from_text(txt_paths,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_not_mask=f'{path}{file_to_train}_results/{file_to_save}/final_not_masked_text.txt'\n",
    "with open(path_to_not_mask,'a',encoding=\"utf8\") as f:\n",
    "    f.write(f'{text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=''\n",
    "for file_path in txt_paths:\n",
    "    print(file_path)\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "        text=text+data\n",
    "        data=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_mask(evaluation_list,proc_dspan):\n",
    "    max_value = max(evaluation_list)\n",
    "    indices = [i for i, val in enumerate(evaluation_list) if val == max_value]\n",
    "    mask_id=random.choice(indices)\n",
    "    return proc_dspan[mask_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m source \u001b[38;5;241m=\u001b[39m [(match\u001b[38;5;241m.\u001b[39mgroup(), match\u001b[38;5;241m.\u001b[39mstart(), match\u001b[38;5;241m.\u001b[39mend()) \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfinditer(pattern, text)]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m source:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mW+$\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m w[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINK\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m w[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m𡨸\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     number\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/regex/regex.py:253\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags, pos, endpos, partial, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    250\u001b[0m   concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning a match\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     pat \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mmatch(string, pos, endpos, concurrent, partial, timeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/regex/regex.py:454\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags, ignore_unused, kwargs, cache_it)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m DEFAULT_VERSION\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_VERSION\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1017\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_freq=[]\n",
    "list_word_freq=[]\n",
    "masked_dspan_all=[]\n",
    "number=0\n",
    "full_word_freq = Counter()\n",
    "\n",
    "pattern = r\"\\w+('\\w+)?|[^\\w\\s]\"\n",
    "source = [(match.group(), match.start(), match.end()) for match in re.finditer(pattern, text)]\n",
    "\n",
    "for w in source:\n",
    "    if not w[0] or re.match(r'^\\W+$', w[0]) or w[0] in [\"URL\",\"LINK\",\"TEL\"] or w[0] =='𡨸':\n",
    "        continue\n",
    "    number+=1\n",
    "    word_freq = Counter([w[0]])\n",
    "    full_word_freq.update(word_freq) \n",
    "    list_freq.append(full_word_freq[w[0]])\n",
    "    dspan=(w[1],w[2])\n",
    "    list_word_freq.append(dspan)\n",
    "    if number==4:\n",
    "        masked_dspan=select_mask(list_freq,list_word_freq)\n",
    "        masked_dspan_all.append(masked_dspan)\n",
    "        list_word_freq=[]\n",
    "        list_freq=[]\n",
    "        number=0\n",
    "masked_dspan_all.insert(0,(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masked_dspan_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_words=[]\n",
    "masked_text=''\n",
    "path_to_save_masked_text=f'{path}{file_to_train}_results/{file_to_save}/final_masked_text.txt'\n",
    "path_to_save_masked_words=f'{path}{file_to_train}_results/{file_to_save}/masked_words.txt'\n",
    "\n",
    "last_mask= masked_dspan_all[-1][-1]\n",
    "len_text= len(text)\n",
    "\n",
    "\n",
    "len_text,len_text\n",
    "for i_dspan in range(1,len(masked_dspan_all)):\n",
    "    prev_start, prev_end = masked_dspan_all[i_dspan-1]\n",
    "    start, end = masked_dspan_all[i_dspan]\n",
    "    count_mask=len(hf_tokenizer.encode_plus(text[start:end], add_special_tokens=False, return_offsets_mapping=True)['input_ids'])\n",
    "    mask_words.append(text[start:end])\n",
    "    part_for_masking=text[prev_end:end]\n",
    "    index = part_for_masking.rfind(text[start:end])\n",
    "    mask=\"[MASK]\"*count_mask\n",
    "    part_text=part_for_masking[:index] + mask + part_for_masking[index + len(text[start:end]):]\n",
    "    masked_text+= part_text\n",
    "masked_text=masked_text+text[last_mask:len_text]\n",
    "\n",
    "with open(path_to_save_masked_text,'a',encoding=\"utf8\") as f:\n",
    "    f.write(masked_text)\n",
    "with open(path_to_save_masked_words, 'w',encoding=\"utf8\") as output:\n",
    "    for row in mask_words:\n",
    "        output.write(str(row) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
